# -*- coding: utf-8 -*-
"""Brain_Stroke.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gcv5_rXGyC4GPXF1hAztIqYUR4t7fIF0

**BRAIN STROKE PREDICTION MODEL USING AI-ML**

**Importing the dependencies/libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the necessary libraries for data manipulation and visualization

import numpy as np # For numerical operations and array manipulation
import pandas as pd # For handling and analyzing data in tabular format
import matplotlib.pyplot as plt # For creating static, animated, and interactive visualizations

# Ensures that matplotlib plots are displayed inline in Google Colab
# %matplotlib inline
import seaborn as sns # For advanced and aesthetically pleasing data visualizations

# Setting default figure size for all plots to improve readability
plt.rcParams["figure.figsize"] = (10,10)

# Mounting Google Drive to access datasets stored in the Drive
from google.colab import drive
drive.mount("/content/drive")

"""**Data Collection**"""

#Importing the dataset
dataset = pd.read_csv("/content/drive/My Drive/Colab Data/healthcare-dataset-stroke-data (1).csv")

#Printing first five rows of the dataset
dataset.head()

#Printing last five rows of the dataset
dataset.tail()

#Getting the dimension of the dataset
dataset.shape

"""**Preprocessing the Data**"""

#Checking for any duplicate rows in the dataset
dataset.duplicated().sum()

#Getting some information about the dataset
dataset.info()

"""**Handling Missing Values**"""

#Checking for any missing values in any column
dataset.isnull().sum()

"""Now as we can see our data has 201 null values in the "bmi" column. So we will handle these null values

**KNN Imputation Technique**
"""

from sklearn.impute import KNNImputer

# Create a KNN imputer object with k=5 (you can adjust this value)
imputer = KNNImputer(n_neighbors=5)

# Select the columns with missing values ('bmi' in this case)
columns_to_impute = ['bmi']

# Fit and transform the imputer on the selected columns
dataset[columns_to_impute] = imputer.fit_transform(dataset[columns_to_impute])

# #Filling the empty/null values of "bmi" with the mean value
# dataset.fillna({"bmi": dataset["bmi"].mean()}, inplace=True)

"""**Rechecking if still any Missing Values**"""

#Checking again if is there any missing value stil
dataset.isnull().sum()

"""**Feature Selection**"""

#Dropping the unwanted column or which is not necessary or required for predicting target value
dataset.drop("id", axis = 1, inplace = True)

#Getting the dimension of the updated dataset
dataset.shape

#Dataset used in the model
dataset

"""**Exploratory Data Analysis: Feature Distributions**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'dataset' is your DataFrame as defined in the provided code.

# List of features to plot
features = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',
            'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']

# Create subplots
num_features = len(features)
num_cols = 4  # Number of columns in the subplot grid
num_rows = (num_features + num_cols - 1) // num_cols

fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 3 * num_rows))
axes = axes.flatten() # Flatten the axes array for easy iteration

for i, feature in enumerate(features):
    ax = axes[i]
    if dataset[feature].dtype == 'object':  # Categorical feature
        sns.countplot(x=feature, hue='stroke', data=dataset, ax=ax)
    else:  # Numerical feature
        sns.histplot(data=dataset, x=feature, hue='stroke', kde=True, ax=ax, element="step")

    ax.set_title(f'Distribution of {feature}')
    ax.set_xlabel(feature)
    ax.set_ylabel('Count')

# Remove any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""**Outlier Removal**"""

dataset.plot(kind = "box")

dataset["avg_glucose_level"].describe()

#Justification for not removing the outliers
plt.figure(figsize=(8, 5))
sns.boxplot(x='stroke', y='avg_glucose_level', data=dataset)
plt.title('Average Glucose Level vs. Stroke')
plt.show()

plt.figure(figsize=(8, 5))
sns.histplot(dataset['avg_glucose_level'], kde=True)
plt.title('Distribution of Average Glucose Levels')
plt.show()

print("Justification for not removing outliers in 'avg_glucose_level':")
print("1. Physiological Variability: Glucose levels naturally vary significantly among individuals.")
print("    Extreme values might represent genuine cases of hyperglycemia or other conditions, which can be")
print("    strong indicators for stroke risk.")
print("2. Clinical Relevance:  High or low glucose levels are medically significant. Removing these outliers")
print("    could remove valuable data points that contribute to understanding stroke risk.")
print("3. Visual Inspection (Boxplot and Histogram): The boxplot and histogram show that the distribution of glucose")
print("     levels has a wide spread and a few extreme values.  Removing these might distort the true relationship")
print("     between glucose levels and stroke prediction. The boxplot specifically shows that there are high glucose")
print("     levels present in both people who had a stroke and those who did not, indicating a complex relationship.")
print("4. Model Robustness:  Including the outliers in the dataset and using appropriate model regularization")
print("    techniques can allow the model to learn the subtle patterns in the data without being unduly influenced")
print("    by extreme values.")

"""**Encoding Categorial Data**"""

# Before encoding
dataset.info()

"""**Label Encoding**"""

# Encoding categorial data into integer by Label encoding method
from sklearn.preprocessing import LabelEncoder
enc = LabelEncoder()

gender = enc.fit_transform(dataset["gender"])
print(gender)
# Female = 0
# Male = 1

ever_married = enc.fit_transform(dataset["ever_married"])
print(ever_married)
# Yes = 1
# No = 0

work_type = enc.fit_transform(dataset["work_type"])
print(work_type)
# Govt_job = 0
# Children = 1
# Private = 2
# Self-employed = 3

Residence_type = enc.fit_transform(dataset["Residence_type"])
print(Residence_type)
# Rural = 0
# Urban = 1

smoking_status = enc.fit_transform(dataset["smoking_status"])
print(smoking_status)
# unknown = 0
# formerly smoked = 1
# never smoked = 2
# smokes = 3

dataset["gender"] = gender
dataset["ever_married"] = ever_married
dataset["work_type"] = work_type
dataset["Residence_type"] = Residence_type
dataset["smoking_status"] = smoking_status

# After Label Encoding
dataset.info()

# So after encoding the categorial values into integers the dataset looks like this
dataset

# Correlation matrix
correlation_matrix = dataset.corr()

# Heatmap
sns.heatmap(correlation_matrix, annot=True, cmap="Spectral")
plt.show()

"""**Splitting the Data for Training and Testing**"""

#Dropping the target/output variable column
X = dataset.drop("stroke", axis = 1)
X

#Target variable column
Y = dataset["stroke"]
Y

#Splitting the data into 80:20 (Training:Testing) ratio
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 101)

#Training data
X_train

#Training Data of target variable
Y_train

#Testing Data
X_test

#Testing Data of target variable
Y_test

"""**Scaling the data**

"""

dataset.describe()

# Importing the StandardScaler from sklearn for data Standardization
from sklearn.preprocessing import StandardScaler
std = StandardScaler() # Initializing the scaler

# Applying standardization to the training and test datasets
X_train_std = std.fit_transform(X_train)
X_test_std = std.transform(X_test)

# Displaying the standardized training data
X_train_std

# Displaying the standardized test data
X_test_std

"""**MODEL TRAINING**

**K-Nearest Neighbour**
"""

# Importing the K-Nearest Neighbors (KNN) classifier from sklearn
# This is the initial training model with manually chosen hyperparameters

from sklearn.neighbors import KNeighborsClassifier # Explain in report
knn = KNeighborsClassifier(n_neighbors = 3, weights = 'uniform', metric = 'euclidean' , algorithm = 'kd_tree')
knn.fit(X_train_std, Y_train)
knn_predicted = knn.predict(X_test_std)

"""**Model Evaluation**"""

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, log_loss

#Confusion matrix ,accuracy score and classification report
knn_conf_matrix = confusion_matrix(Y_test, knn_predicted)
knn_acc_score = accuracy_score(Y_test, knn_predicted)

print("Confusion Matrix:")
print(knn_conf_matrix)
print("\n")
# print("Accuracy of the KNN model: ", knn_acc_score*100, "%")

#Evaluating accuracy of the training data
X_train_prediction = knn.predict(X_train_std)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
print("Accuracy score of the training data: ", training_data_accuracy*100, "%")

#Evaluating accuracy of the testing data
X_test_prediction = knn.predict(X_test_std)
testing_data_accuracy = accuracy_score(X_test_prediction, Y_test)
print("Accuracy score of the testing data: ", testing_data_accuracy*100, "%")

#Classification report
print("\n")
print("Classification report:")
print(classification_report(Y_test, knn_predicted, zero_division = 0))

#Misclassification rate
print("\n")
misclassification_rate = 1 - knn_acc_score
print(f"Misclassification Rate: {misclassification_rate * 100:.2f}%")

# Binary Cross Entropy Loss (Log Loss)
try:
    # Predicting probabilities for the positive class (stroke)
    knn_probabilities = knn.predict_proba(X_test_std)[:, 1]

    # Calculate binary cross-entropy loss (log loss)
    binary_cross_entropy = log_loss(Y_test, knn_probabilities)
    print(f"\nBinary Cross-Entropy Loss: {binary_cross_entropy:.4f}")

except AttributeError:
    print("\nWarning: KNN model does not provide probability estimates. Cannot calculate binary cross-entropy loss.")

"""As we can see from the clssification report that our data is highly imbalance so now i will apply SMOTE technique for data balancing

**Data Handling by SMOTE**
"""

from imblearn.over_sampling import SMOTE

# Separate features (X) and target (y)
X = dataset.drop("stroke", axis=1)
y = dataset["stroke"]

# Apply SMOTE
smote = SMOTE(random_state=42)  # You can adjust the random_state
X_resampled, y_resampled = smote.fit_resample(X, y)

# Convert the resampled data back to a DataFrame
X_resampled = pd.DataFrame(X_resampled, columns=X.columns)
dataset_resampled = pd.concat([X_resampled, pd.Series(y_resampled, name="stroke")], axis=1)

X_resampled.shape

y_resampled.shape

"""**Splitting the oversampling data**"""

# Now, use X_resampled and y_resampled for train_test_split and model training.
X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 69)

#Standardization
std = StandardScaler()
X_train_std = std.fit_transform(X_train)
X_test_std = std.transform(X_test)

print(X_train.shape) # Training data
print(X_test.shape) # Training label
print(Y_train.shape) # Testing data
print(Y_test.shape) # Testing label

"""**Model Optimization by Grid Search for the balanced data**"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 10],  # Example values of n
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'] # Example algorithms
}

# Create the KNN classifier
knn = KNeighborsClassifier()

# Create the GridSearchCV object
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy') # Adjust cv as needed

# Fit the GridSearchCV object to the data
grid_search.fit(X_train_std, Y_train)

# Print the best hyperparameters and best score
print("Best hyperparameters:", grid_search.best_params_)
print("Best score for traiining data:", grid_search.best_score_)

# Get the best model
best_knn = grid_search.best_estimator_

# Make predictions on the test data using the best model
best_knn_predicted = best_knn.predict(X_test_std)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

results = pd.DataFrame(grid_search.cv_results_)

# Plotting the results
for param in ['param_n_neighbors', 'param_weights', 'param_metric', 'param_algorithm']:  # Adjust based on your param_grid
    plt.figure(figsize=(8, 5))
    sns.lineplot(x=param, y='mean_test_score', data=results)
    plt.title(f'Mean Test Score vs. {param.replace("param_", "")}')
    plt.xlabel(param.replace("param_", ""))
    plt.ylabel('Mean Test Score')
    plt.show()

"""**Model Evaluation after SMOTE & Grid Search for balanced data**"""

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, log_loss

# Confusion matrix, accuracy score, and classification report
best_knn_conf_matrix = confusion_matrix(Y_test, best_knn_predicted)
best_knn_acc_score = accuracy_score(Y_test, best_knn_predicted)

print("Confusion Matrix:")
print(best_knn_conf_matrix)
print("\n")
# print("Accuracy of the best KNN model: ", best_knn_acc_score * 100, "%")

# # Evaluating accuracy of the training data
# X_train_prediction = best_knn.predict(X_train_std)
# training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
# print("Accuracy score of the training data: ", training_data_accuracy * 100, "%")

# # Evaluating accuracy of the testing data
# X_test_prediction = best_knn.predict(X_test_std)
# testing_data_accuracy = accuracy_score(X_test_prediction, Y_test)
# print("Accuracy score of the testing data: ", testing_data_accuracy * 100, "%")

# Classification report
print("\n")
print("Classification report:")
print(classification_report(Y_test, best_knn_predicted))

# Misclassification rate
print("\n")
misclassification_rate = 1 - best_knn_acc_score
print(f"Misclassification Rate: {misclassification_rate * 100:.2f}%")

# Binary Cross Entropy Loss (Log Loss)
try:
    # Predicting probabilities for the positive class (stroke)
    best_knn_probabilities = best_knn.predict_proba(X_test_std)[:, 1]

    # Calculate binary cross-entropy loss (log loss)
    binary_cross_entropy = log_loss(Y_test, best_knn_probabilities)
    print(f"\nBinary Cross-Entropy Loss: {binary_cross_entropy:.4f}")

except AttributeError:
    print("\nWarning: KNN model does not provide probability estimates. Cannot calculate binary cross-entropy loss.")

"""**Average Accuracy for testing data (5-fold cross-validation)**"""

from sklearn.model_selection import cross_val_score

# 'best_knn' as the optimized KNN model and 'X_test_std', 'Y_test' as testing data
cv_scores = cross_val_score(best_knn, X_test_std, Y_test, cv=5, scoring='accuracy') # 5-fold cross-validation

# Calculate the average accuracy
average_accuracy = np.mean(cv_scores)
print(f"Average Accuracy for testing data (5-fold cross-validation): {average_accuracy * 100:.2f}%")

# You can also print the individual fold scores if needed:
print("Accuracy scores for each fold:", cv_scores)

"""**Model Deployment**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier

def predict_stroke(gender, age, hypertension, heart_disease, ever_married, work_type, Residence_type, avg_glucose_level, bmi, smoking_status):
    """Predicts the probability of stroke based on input features."""

    # Create a DataFrame from user input (mimicking one row of your dataset)
    input_data = pd.DataFrame({
        'gender': [gender],
        'age': [age],
        'hypertension': [hypertension],
        'heart_disease': [heart_disease],
        'ever_married': [ever_married],
        'work_type': [work_type],
        'Residence_type': [Residence_type],
        'avg_glucose_level': [avg_glucose_level],
        'bmi': [bmi],
        'smoking_status': [smoking_status]
    })

    # Encoding categorical features using LabelEncoder (same as training)
    enc = LabelEncoder()
    for column in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:
      input_data[column] = enc.fit_transform(input_data[column]) # Fit and transform on the input data

    # Normalize the input data using the same StandardScaler
    input_data_scaled = std.transform(input_data)

    # Make the prediction
    prediction = best_knn.predict(input_data_scaled)

    return prediction[0]  # Return the prediction

# Example Usage: Get User Input
gender = input("Enter gender (Male/Female/Other): ")
age = float(input("Enter age: "))
hypertension = int(input("Enter hypertension (0 or 1): "))
heart_disease = int(input("Enter heart disease (0 or 1): "))
ever_married = input("Enter marital status (Yes/No): ")
work_type = input("Enter work type (Govt_job/children/Private/Self-employed/Never_worked): ")
Residence_type = input("Enter residence type (Urban/Rural): ")
avg_glucose_level = float(input("Enter average glucose level: "))
bmi = float(input("Enter BMI: "))
smoking_status = input("Enter smoking status (formerly smoked/never smoked/smokes/unknown): ")

# Make the prediction
prediction = predict_stroke(gender, age, hypertension, heart_disease, ever_married, work_type, Residence_type, avg_glucose_level, bmi, smoking_status)

# # Print the results
# print(f"Predicted stroke: {prediction}")

# Print the results with user-friendly message
print('\n')
print(f"Prediction = {prediction}")
# if prediction == 1:
#     print("There is a chance of stroke based on the input data. Please consult a healthcare professional for further evaluation.")
# else:
#     print("There is no significant chance of stroke based on the input data. However, maintaining a healthy lifestyle is always recommended.")

"""**Average Loss**"""

# 'best_knn' as the optimized KNN model and 'X_test_std', 'Y_test' as testing data

try:
    best_knn_probabilities = best_knn.predict_proba(X_test_std)[:, 1]
    binary_cross_entropy = log_loss(Y_test, best_knn_probabilities)
    print(f"\nBinary Cross-Entropy Loss for testing data: {binary_cross_entropy:.4f}")
except AttributeError:
    print("\nWarning: KNN model does not provide probability estimates. Cannot calculate binary cross-entropy loss.")

"""**Permutation Feature Importance**"""

import numpy as np
from sklearn.inspection import permutation_importance

# Calculate permutation feature importance
result = permutation_importance(best_knn, X_test_std, Y_test, n_repeats=30, random_state=69)

# Get feature importances
importances = result.importances_mean
std = result.importances_std

# Create a DataFrame for easier visualization
feature_importances = pd.DataFrame({'feature': X.columns, 'importance': importances, 'std': std})
feature_importances = feature_importances.sort_values(by='importance', ascending=False)


# Plot feature importances
plt.figure(figsize=(10, 6))
plt.bar(feature_importances['feature'], feature_importances['importance'], yerr=feature_importances['std'])
plt.xlabel('Feature')
plt.ylabel('Permutation Importance')
plt.title('Feature Importance for KNN Model')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability
plt.tight_layout()
plt.show()

feature_importances

